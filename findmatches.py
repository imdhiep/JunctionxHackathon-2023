# -*- coding: utf-8 -*-
"""findMatches.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJH8Y01qlyApMwm11ZnHywa5ceBn86CM
"""

import numpy as np
import cv2

# load the images
image1 = cv2.imread('/content/1.png')
image2 = cv2.imread('/content/2.png')

def findMatches(image1, image2):
  #img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
  #img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

  # Initiate SIFT detector
  sift = cv2.SIFT_create()

  # find the keypoints and descriptors with SIFT
  keypoint1, descriptors1 = sift.detectAndCompute(image1, None)
  keypoint2, descriptors2 = sift.detectAndCompute(image2, None)

  # finding nearest match with KNN algorithm
  index_params = dict(algorithm=0, trees=20)
  search_params = dict(checks=150) # or pass empty dictionary

  # Initialize the FlannBasedMatcher
  flann = cv2.FlannBasedMatcher(index_params, search_params)

  Matches = flann.knnMatch(descriptors1, descriptors2, k=2)

  # Need to draw only good matches, so create a mask
  good_matches = [[0, 0] for i in range(len(Matches))]

  # Ratio Test
  for i, (m, n) in enumerate(Matches):
    if m.distance < 0.65*n.distance:
      good_matches[i] = [1, 0]

  # Draw the matches using drawMatchesKnn()
  # Matched = cv2.drawMatchesKnn(image1,
  #               keypoint1,
  #               image2,
  #               keypoint2,
  #               Matches,
  #               outImg=None,
  #               matchColor=(0, 155, 0),
  #               singlePointColor=(0, 0, 255),
  #               matchesMask=good_matches,
  #               flags=0
  #               )

  # Displaying the image
  # cv2.imwrite('Match.jpg', Matched)
  return Matches

findMatches(image1, image2)

# Commented out IPython magic to ensure Python compatibility.
import cv2 
import matplotlib.pyplot as plt
# %matplotlib inline

# read images
img1 = cv2.imread('/content/1.png')  
img2 = cv2.imread('/content/2.png') 

img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

#sift
sift = cv2.xfeatures2d.SIFT_create()

keypoints_1, descriptors_1 = sift.detectAndCompute(img1,None)
keypoints_2, descriptors_2 = sift.detectAndCompute(img2,None)

#feature matching
bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)

matches = bf.match(descriptors_1,descriptors_2)
matches = sorted(matches, key = lambda x:x.distance)

img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:50], img2, flags=2)
plt.imshow(img3),plt.show()

import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt
img1 = cv.imread('/content/1.png',cv.IMREAD_GRAYSCALE)          # queryImage
img2 = cv.imread('/content/2.png',cv.IMREAD_GRAYSCALE) # trainImage
# Initiate SIFT detector
sift = cv.SIFT_create()
# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
# FLANN parameters
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)   # or pass empty dictionary
flann = cv.FlannBasedMatcher(index_params,search_params)
matches = flann.knnMatch(des1,des2,k=2)
# Need to draw only good matches, so create a mask
matchesMask = [[0,0] for i in range(len(matches))]
# ratio test as per Lowe's paper
pts1 = []
pts2 = []
# ratio test as per Lowe's paper
for i,(m,n) in enumerate(matches):
    if m.distance < 0.8*n.distance:
        pts2.append(kp2[m.trainIdx].pt)
        pts1.append(kp1[m.queryIdx].pt)

test1 = pts1

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt
img1 = cv.imread('/content/1.png',cv.IMREAD_GRAYSCALE)          # queryImage
img2 = cv.imread('/content/3.png',cv.IMREAD_GRAYSCALE) # trainImage
# Initiate SIFT detector
sift = cv.SIFT_create()
# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
# FLANN parameters
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)   # or pass empty dictionary
flann = cv.FlannBasedMatcher(index_params,search_params)
matches = flann.knnMatch(des1,des2,k=2)
# Need to draw only good matches, so create a mask
matchesMask = [[0,0] for i in range(len(matches))]
# ratio test as per Lowe's paper
pts1 = []
pts2 = []
# ratio test as per Lowe's paper
for i,(m,n) in enumerate(matches):
    if m.distance < 0.8*n.distance:
        pts2.append(kp2[m.trainIdx].pt)
        pts1.append(kp1[m.queryIdx].pt)

test2 = pts1



sl = 0
for i in test1:
  for j in test2:
    if(i == j): sl+=1
sl

hull = []
contours = pts1
# calculate points for each contour
# for i in range(len(contours)):
    # creating convex hull object for each contour
    # hull.append(cv.convexHull(contours[i], False))
hull = cv.convexHull(np.array(pts1,dtype='float32'))
x = [i[0][0] for i in hull]
y = [i[0][1] for i in hull]

points = [i[0] for i in hull]

hull.shape

cv2.drawContours(img1, [hull], -1, (0,255,255), 3)

src = cv2.imread("/content/1.png", 1) 
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY) # convert to grayscale
blur = cv2.blur(gray, (3, 3)) # blur the image
ret, thresh = cv2.threshold(blur, 50, 255, cv2.THRESH_BINARY)
	
# Finding contours for the thresholded image
im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
hull = []
 
# calculate points for each contour
for i in range(len(contours)):
    # creating convex hull object for each contour
    hull.append(cv2.convexHull(contours[i], False))
hull